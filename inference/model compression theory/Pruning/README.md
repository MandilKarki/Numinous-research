# Pruning

- Pruning is a way to remove unnecessary weights or neurons from a neural network, making it smaller and faster without sacrificing accuracy. We can often prune up to [90% of the parameters](https://arxiv.org/abs/1506.02626) in a large deep neural network without any noticeable loss in performance.
- Model pruning is a way to remove unnecessary weights from a neural network by understanding which weights are important to the model's performance. This can be done by analyzing the impact of each weight on the loss function, using regularization methods (L1 and L2), or removing entire neurons or layers. The goal of pruning is to create a smaller, more efficient model without sacrificing accuracy.
- The [lottery ticket hypothesis](https://arxiv.org/abs/1803.03635) is a theory that explains why model pruning works. It states that every neural network contains a subnetwork that is capable of achieving the same accuracy as the original network, even if it is much smaller. This subnetwork is called a "winning ticket."
    - For example: Let's say we have a tabular dataset with 100 features and 10 classes. We want to train a neural network to predict the class of each data point.

        We could train a large neural network with 1000 neurons in the hidden layer. This network would have 100,000 parameters. However, according to the lottery ticket hypothesis, this network contains a subnetwork with only a few thousand parameters that can be trained to achieve the same accuracy as the original network.

        We could find this subnetwork by using a technique called pruning. Pruning is a process of removing the less important parameters from a neural network. This can be done by iteratively removing the parameters with the smallest weights or by removing the parameters that have the least impact on the network's performance.

        In this example, we could use pruning to remove 95% of the parameters from the original network. This would leave us with a smaller network with only 5000 parameters. However, this smaller network would still be able to achieve the same accuracy as the original network.

    - Randomly initialized neural networks contain subnetworks that can be trained to achieve similar accuracy to the original network, even if they are much smaller. This is known as the lottery ticket hypothesis.

> Large neural networks contain subnetworks that can be trained to achieve the same accuracy as the original network, even if they are much smaller.

## Structured vs. Unstructured Pruning
- Structured pruning removes neurons or chooses a subnetwork, while unstructured pruning sparsifies model weights using methods such as TensorFlow's `tensorflow_model_optimization` and PyTorch's `torch.nn.utils.prune`. This can save disk space using compression algorithms such as run-length encoding or [byte-pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) and it may also speed up inference when sparse model support is fully implemented in various frameworks because multiplying a sparse vector and a sparse matrix is faster than multiplying a dense vector and a dense matrix.
- Structured pruning, a dynamic research field lacking a clear API, involves selecting a metric to assess the significance of each neuron. Subsequently, neurons with lower information content can be pruned, with potentially useful metrics encompassing the [Shapley value](https://christophm.github.io/interpretable-ml-book/shapley.html), a Taylor approximation measuring a neuron's impact on loss sensitivity, or even random selection. Notably, the [TorchPruner](https://github.com/marcoancona/TorchPruner) library automatically incorporates some of these metrics for `nn.Linear` and convolution modules, while the [Torch-Pruning](https://github.com/vainf/torch-pruning) library offers support for additional operations. Among the notable earlier contributions, one involves filter pruning in convnets using the L1 norm of filter weights.
- Unstructured pruning is a technique for reducing the size of a neural network by zeroing out weights with small magnitudes. It can be done during or after training, and the target sparsity can be adjusted to achieve the desired balance between model size and accuracy. However, [there is some confusion](https://arxiv.org/abs/2003.03033) in this area, so it is important to consult the documentation for [TensorFlow](https://www.tensorflow.org/model_optimization/guide/pruning/) and [PyTorch](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html) before using unstructured pruning.

## Fine Tuning | [What is Fine Tuning](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/tree/main/Articles/Training/Fine%20Tuning%20Models)
- After pruning a neural network, it is [standard practice](https://arxiv.org/pdf/2003.02389.pdf) to retrain the network. The best method is to reset the learning rate to its original value and start training from scratch. Optionally, you can also reset the weights of the unpruned parts of the network to their values earlier in training. This is essentially training the lottery ticket subnetwork that we have identified.
    - For example: let's say we have a neural network with 1000 weights. We use pruning to remove 90% of the weights, leaving us with 100 weights. We then retrain the network with a reset learning rate and the weights from the earlier training. This helps the lottery ticket subnetwork to learn how to perform the task at hand more effectively.

>  If you are interested in network pruning, you can start by using [TorchPruner](https://github.com/marcoancona/TorchPruner) or [Torch-Pruning](https://github.com/vainf/torch-pruning) to prune the network. Then, you can fine-tune the resulting network with learning rate rewinding. However, it is not always clear how to trim the rest of the network around the pruned part, especially for architectures with skip connections like ResNets.
