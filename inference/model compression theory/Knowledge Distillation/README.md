# Knowledge Distillation
- Knowledge distillation is a technique for transferring knowledge from a large model (teacher) to a smaller model (student), resulting in smaller and more efficient models. [Hinton et al., 2015](https://arxiv.org/abs/1503.02531)
- "Knowledge distillation is a process of transferring knowledge from a large model (teacher) to a smaller model (student). The student model can learn to produce similar output responses (response-based distillation), reproduce similar intermediate layers (feature-based distillation), or reproduce the interaction between layers (relation-based distillation)." [aiedge.io](https://newsletter.theaiedge.io/)
- The image below, which is sourced from [AiEdge.io](https://newsletter.theaiedge.io/), does an excellent job of visualizing the concept of knowledge distillation.
![knowledge_distilation](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/assets/40186859/205a960c-c1ce-4d18-9c3a-feab2df12f45)
- Knowledge distillation is a technique that allows us to deploy large deep learning models in production by training a smaller model (student) to mimic the performance of a larger model (teacher).
> The key idea of knowledge distillation is to train the student model with the soft target of the teacher model's output probability distribution, instead of the same labeled data as the teacher.
- During a standard training process, the teacher model learns to discriminate between many classes by maximizing the probability of the correct label. This side effect, where the model assigns smaller probabilities to other classes, can give us valuable insights into how the model generalizes. For example, an image of a cat is more likely to be mistaken for a tiger than a chair, even though the probability of both mistakes is low. We can use this knowledge to train a student model that is more accurate and robust.
- The student model is typically a smaller version of the teacher model, with fewer parameters. However, it is recommended to use the same network structure as the teacher model, as this can help the student model to learn more effectively. For example, if we want to use BERT as a teacher model, we can use DistillBERT, which is a 40% smaller version of BERT with the same network structure.
- The student model is trained to minimize a loss function that is a combination of the teacher's original training loss and a distillation loss. The distillation loss is calculated by taking the teacher's softmax output for the correct class, averaging it with the softmax output of the student model, and then scaling the result with a temperature parameter. The temperature parameter controls how soft the averaging is, with a higher temperature resulting in a softer averaging.
-  The temperature parameter effectively smooths out the probability distribution, reducing the higher probabilities and increasing the smaller ones. This results in a softer distribution that contains more knowledge about the uncertainty of the prediction.
- Knowledge distillation can significantly reduce the latency of a machine learning model, with only a small decrease in accuracy.
- In practice, for a classification task, we can train a smaller student model $f_{\theta}$, where $\theta$ is the set of parameters, by using a large model or an ensemble of models (possibly even the same model trained with different initializations), which we call $F$. We train the student model with the following loss function:
$$\mathcal{L} = \sum\nolimits_{i=1}^n KL(F(x_i),f_{\theta}(x_i))$$

where, $F(x_i)$ = probability distribution over the labels created by passing example $x_i$
 through the network
 - You can optionally add the regular cross-entropy loss to the loss function by passing in the one-hot ground truth distribution to the student model as well. 
 $$\mathcal{L} =  \sum\nolimits_{i=1}^n(KL(F(x_i), f_{\theta}) - \beta.\sum\nolimits_{=1}^K y_i[k] logf_{\theta}(x_i)[k])$$
> Note: The second term in the loss function is the $KL$ Kullback-Leibler divergence from the one-hot distribution of the labels (the "true" distribution) to the student model's distribution, since the one-hot distribution is a special case of the softmax distribution.
- There is no consensus on why knowledge distillation works, but the most compelling explanation is that it is a form of data augmentation. This paper, [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816), provides a good explanation of why this is the case. The paper is focused on the idea of multiple views, and it provides some thought experiments that may help to explain what is happening at a deeper level.

## Distillation Thought Experiment
- A teacher model trained to classify images may have filters that are sensitive to pointy ears. These filters may fire even when the model is presented with an image of a Batman mask, which is not a cat. This suggests that the model thinks the Batman mask looks 10% like a cat.
- A student model trained to match the probability distribution of the teacher will learn that the Batman mask has a 10% probability of being a cat. This information can help the student model recognize cat-like images, even if they are not labeled as cats. This is because the student model is able to learn from the teacher model's mistakes and identify images that are similar to cats, even if they do not have the exact same features. This logic also explains why the student model can sometimes outperform the teacher model, as the student model is able to learn from the teacher model's mistakes and make better predictions.

## Ensembling Thought Experiment
- Ensembles of models (even with the same architecture) can work well because they can learn different features from the same data. For example, in a dataset of cat images, one model might learn to identify cats with pointed ears, another model might learn to identify cats with whiskers, and a third model might learn to identify cats with both features. By combining the predictions of multiple models, ensembles can reduce the risk of overfitting and improve the overall accuracy of image classification.
- Neural networks can learn to recognize features in data, but they may not learn all of the features that are important. For example, a neural network might learn to recognize feature A by seeing image 1. However, if the network only sees image 1 and image 3, it may not learn feature B, even though feature B is also present in image 3. This is because the network will not receive any gradient signal to learn feature B. A good neural network would learn both feature A and feature B, but this may not always happen.
-  A neural network learns to classify data points more accurately, it may become less sensitive to small changes in the data. This is because the network is already able to classify the data points with high confidence, so it does not need to rely on every single data point to make a decision. As a result, the signal from some data points may decrease as the network becomes more accurate.

## Distillation in Practice
- Knowledge distillation is a rapidly growing research field with applications in defending against adversarial attacks, transferring knowledge between models, and protecting privacy.
- In knowledge distillation, the student model learns from the teacher model by mimicking its predictions. Response-based distillation focuses on the final output layer of the teacher model, while feature-based and relation-based distillation focus on other parts of the teacher model.
- The different types of distillation, such as offline distillation (where the student model is trained after the teacher model), online distillation (where the student and teacher models are trained together), and self-distillation (where the teacher model has the same architecture as the student model), can make it difficult to track distillation in practice. A set of ad hoc model-specific techniques may be the best general recommendation.
- In Fact,
    - [Cho & Hariharan (2019)](https://arxiv.org/abs/1910.01348) found that knowledge distillation can be harmful when the student model is too small. They also found that knowledge distillation papers rarely use ImageNet and so often don't work well on difficult problems.
    - [Mirzadeh et al. (2019)](https://arxiv.org/abs/1902.03393) found that better teacher models don't always mean better distillation, and that the farther the student and teacher model's capacities are, the less effective distillation is.
    - A recent investigation by [Tang et al. (2021)](https://arxiv.org/pdf/2002.03532.pdf) supports these findings.
> Note: Knowledge distillation can be harmful when the student model is too small, and it is less effective when the student and teacher models have different capacities.
- In Summary, knowledge distillation is a powerful technique for improving the performance of small models, but it is more difficult to implement than quantization and pruning. However, it can be worth the effort if you need to achieve a high level of accuracy with a small model.

### Distillation As Semi-supervised Learning
- A teacher model can be used to transfer knowledge to a student model. The teacher model is first trained on a large set of labeled data. Then, it is used to generate soft labels for a smaller set of unlabeled data. These soft labels can then be used to train the student model. This approach allows the student model to learn from the knowledge of the teacher model, even though it is not trained on as much data.
- [Parthasarathi and Strom (2019)](https://arxiv.org/pdf/1904.01624.pdf) used a two-step approach to train an acoustic model for speech recognition. First, they trained a powerful teacher model on a small set of annotated data. This teacher model was then used to label a much larger set of unannotated data. Finally, they trained a leaner, more efficient student model on the combined dataset of annotated and unlabeled data.

![Distillation As Semi-supervised Learning](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/assets/40186859/32d6f8a2-e9aa-4fa0-b485-7a1f1e648320)
