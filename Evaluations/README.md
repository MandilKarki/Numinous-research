The Evaluations folder contains an array of assessment frameworks and metrics designed for scrutinizing the performance of Large Language Models. It encapsulates tools for both quantitative and qualitative analysis, ensuring comprehensive understanding and validation of LLMs against industry-standard benchmarks in natural language understanding and generation.


1) DeepEval - The DeepEval GitHub repository provides an open-source evaluation framework for Large Language Models (LLMs). It's designed for Python and allows users to build and iterate LLM applications efficiently. The repository includes guidelines for setting up the Python environment, installing DeepEval, creating test cases, and custom metrics for evaluating LLM outputs. DeepEval simplifies unit testing of LLM outputs and supports a variety of evaluation metrics and datasets, making it a comprehensive tool for both development and production environments. For more details and usage instructions, you can visit the repository here.








1) Results of WMT23 Metrics Shared Task: Metrics might be Guilty but References are not Innocent - https://www2.statmt.org/wmt23/pdf/2023.wmt-1.51.pdf
2) https://aclanthology.org/2023.wmt-1.63.pdf - Proceedings of the Eighth Conference on Machine Translation (WMT), pages 756–767
December 6–7, 2023. ©2023 Association for Computational Linguistics
756MetricX-23: The Google Submission to the WMT 2023
Metrics Shared Task
metricsX - https://huggingface.co/google/metricx-23-xxl-v2p0

