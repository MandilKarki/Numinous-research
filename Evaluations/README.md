The Evaluations folder contains an array of assessment frameworks and metrics designed for scrutinizing the performance of Large Language Models. It encapsulates tools for both quantitative and qualitative analysis, ensuring comprehensive understanding and validation of LLMs against industry-standard benchmarks in natural language understanding and generation.

Table of Contents -
1) DeepEval
2) ðŸ“Š  Evals from OpenAI
   

   
Introduction
1) DeepEval - The DeepEval GitHub repository provides an open-source evaluation framework for Large Language Models (LLMs). It's designed for Python and allows users to build and iterate LLM applications efficiently. The repository includes guidelines for setting up the Python environment, installing DeepEval, creating test cases, and custom metrics for evaluating LLM outputs. DeepEval simplifies unit testing of LLM outputs and supports a variety of evaluation metrics and datasets, making it a comprehensive tool for both development and production environments. For more details and usage instructions, you can visit the repository [here](https://github.com/confident-ai/deepeval) .
   
2) ðŸ“Š  Evals from OpenAI: Simplifying and Streamlining LLM Evaluation [link](https://arize.com/blog-course/evals-openai-simplifying-llm-evaluation/#how-to-run-eval)   how to run an evaluation using the oaieval command line interface, the process of building custom evaluations from templates, and creating completion functions for more accurate model responses. The post emphasizes the advantages of using the Eval framework, such as standardized evaluation metrics, ease of use, flexibility, and being open-source.








1) Results of WMT23 Metrics Shared  Task: Metrics might be Guilty but References are not Innocent - https://www2.statmt.org/wmt23/pdf/2023.wmt-1.51.pdf
2) https://aclanthology.org/2023.wmt-1.63.pdf - Proceedings of the Eighth Conference on Machine Translation (WMT), pages 756â€“767
December 6â€“7, 2023. Â©2023 Association for Computational Linguistics
756MetricX-23: The Google Submission to the WMT 2023
Metrics Shared Task
metricsX - https://huggingface.co/google/metricx-23-xxl-v2p0

