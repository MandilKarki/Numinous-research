The Evaluations folder contains an array of assessment frameworks and metrics designed for scrutinizing the performance of Large Language Models. It encapsulates tools for both quantitative and qualitative analysis, ensuring comprehensive understanding and validation of LLMs against industry-standard benchmarks in natural language understanding and generation.








1) Results of WMT23 Metrics Shared Task: Metrics might be Guilty but References are not Innocent - https://www2.statmt.org/wmt23/pdf/2023.wmt-1.51.pdf
2) https://aclanthology.org/2023.wmt-1.63.pdf - Proceedings of the Eighth Conference on Machine Translation (WMT), pages 756–767
December 6–7, 2023. ©2023 Association for Computational Linguistics
756MetricX-23: The Google Submission to the WMT 2023
Metrics Shared Task
metricsX - https://huggingface.co/google/metricx-23-xxl-v2p0

