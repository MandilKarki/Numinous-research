# LLM Training Frameworks

| Framework | Description | Resource |
|------ | ---------- | :--------- |
|Alpa| Alpa is a system for training and serving large-scale neural networks. | [ðŸ”—](https://github.com/alpa-projects/alpa)|
|DeepSpeed| DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. | [ðŸ”—](https://github.com/microsoft/DeepSpeed)|
|Megatron-DeepSpeed| DeepSpeed version of NVIDIA's Megatron-LM that adds additional support for several features such as MoE model training, Curriculum Learning, 3D Parallelism, and others. | [ðŸ”—](https://github.com/microsoft/Megatron-DeepSpeed)|
|FairScale| FairScale is a PyTorch extension library for high performance and large scale training | [ðŸ”—](https://fairscale.readthedocs.io/en/latest/what_is_fairscale.html)|
|Megatron-LM| Ongoing research training transformer models at scale. | [ðŸ”—](https://github.com/NVIDIA/Megatron-LM)|
|Colossal-AI| Making large AI models cheaper, faster, and more accessible. | [ðŸ”—](hhttps://github.com/hpcaitech/ColossalAI)|
|BMTrain | Efficient Training for Big Models. | [ðŸ”—](https://github.com/OpenBMB/BMTrain)|
|Mesh Tensorflow | Mesh TensorFlow: Model Parallelism Made Easier. | [ðŸ”—](https://github.com/tensorflow/mesh)|
|maxtext | A simple, performant and scalable Jax LLM! | [ðŸ”—](https://github.com/google/maxtext)|
|gpt-neox | An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.| [ðŸ”—](https://github.com/EleutherAI/gpt-neox)|
|Trainer API| Provides an API for feature-complete training in PyTorch for most standard use cases | [ðŸ”—](https://huggingface.co/docs/transformers/main_classes/trainer)|
| Lighting | Deep learning framework to train, deploy, and ship AI products Lightning fast | [ðŸ”—](https://github.com/Lightning-AI/lightning)|
| maxtext | A simple, performant and scalable Jax LLM! | [ðŸ”—](https://github.com/google/maxtext)| 